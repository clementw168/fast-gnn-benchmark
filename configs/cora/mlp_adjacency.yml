

parameters:
  num_classes: &num_classes 7
  input_dim: &input_dim 4141


seed: 42
group_id: null
n_runs: 5

data_parameters:
  dataset_type: cora
  # transforms: 
  #   - "normalize_features"
  add_self_loops_and_remove_duplicate_edges: false
  remove_duplicate_edges: false
  train_data_loader_parameters:
    data_loader_type: base_data_loader
    batch_size: 1
    num_workers: 0
    pin_memory: false
    persistent_workers: false
  val_data_loader_parameters:
    data_loader_type: base_data_loader
    batch_size: 1
    num_workers: 0
    pin_memory: false
    persistent_workers: false
  test_data_loader_parameters:
    data_loader_type: base_data_loader
    batch_size: 1
    num_workers: 0
    pin_memory: false
    persistent_workers: false

model_parameters:
  task_type: node_classification
  architecture_parameters:
      architecture_type: mlp_adjacency
      input_dim: *input_dim
      hidden_dim: 512
      output_dim: *num_classes
      num_layers: 1
      dropout: 0.0
      use_residual: false
      use_layer_norm: false
  loss:
      loss_type: cross_entropy
      parameters:
        reduction: none
  metrics:
    - metric_type: optimized_accuracy
      display_name: accuracy
    - metric_type: optimized_f1
      display_name: f1
    - metric_type: optimized_precision
      display_name: precision
    - metric_type: optimized_recall
      display_name: recall
  optimizer:
    optimizer_type: adam
    parameters:
      lr: 0.5
      weight_decay: 0.05

callbacks: 
    # - callback_type: early_stopping
    #   parameters:
    #     monitor: val/loss
    #     patience: 10
    #     mode: "min"
    - callback_type: model_checkpoint
      parameters:
        monitor: val/accuracy
        mode: max
        save_top_k: 1
        filename: best

wandb_logger_parameters:
  reinit: true

trainer_config:
  max_epochs: 100
  log_every_n_steps: 1

