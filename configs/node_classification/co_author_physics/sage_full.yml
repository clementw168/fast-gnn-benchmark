

parameters:
  num_classes: &num_classes 5
  input_dim: &input_dim 8415


seed: 42

data_parameters:
  dataset_type: co-author-physics
  # transforms: 
    # - "normalize_features"
  to_undirected: true
  add_self_loops_and_remove_duplicate_edges: true
  remove_duplicate_edges: false
  train_data_loader_parameters:
    data_loader_type: base_data_loader
    batch_size: 1
    num_workers: 0
    pin_memory: false
    persistent_workers: false
  val_data_loader_parameters:
    data_loader_type: base_data_loader
    batch_size: 1
    num_workers: 0
    pin_memory: false
    persistent_workers: false
  test_data_loader_parameters:
    data_loader_type: base_data_loader
    batch_size: 1
    num_workers: 0
    pin_memory: false
    persistent_workers: false

model_parameters:
  task_type: node_classification
  architecture_parameters:
      architecture_type: sage
      input_dim: *input_dim
      hidden_dim: 128
      output_dim: *num_classes
      num_layers: 4
      dropout: 0.4
      use_input_projection: false
      use_output_projection: false
      use_residual: false
      use_layer_norm: false
  loss:
      loss_type: cross_entropy
      parameters:
        reduction: none
  metrics:
    - metric_type: optimized_accuracy
      display_name: accuracy
    - metric_type: optimized_f1
      display_name: f1
    - metric_type: optimized_precision
      display_name: precision
    - metric_type: optimized_recall
      display_name: recall
  optimizer:
    optimizer_type: adam
    parameters:
      lr: 0.001
      weight_decay: 0.00005

callbacks: 
    # - callback_type: early_stopping
    #   parameters:
    #     monitor: val/loss
    #     patience: 10
    #     mode: "min"
    - callback_type: model_checkpoint
      parameters:
        monitor: val/accuracy
        mode: max
        save_top_k: 1
        filename: best

wandb_logger_parameters:
  reinit: true

compilation_parameters:
  use_compiled_torch: false
  full_graph: false

trainer_config:
  max_epochs: 100
  log_every_n_steps: 1

